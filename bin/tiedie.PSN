#!/usr/bin/env python

###
### TieDIE: Tied Diffusion for Network Discovery
###
###	Version: 
###
###		Multiple (>= 3 inputs) Development Version
###
###	Authors: 
###
###		Evan Paull (epaull@soe.ucsc.edu)
###
###	Requirements:
###
### 	python 2.7.X
###		numpy 1.7+ (with pre-computed kernels)
###		scipy 0.12+ (for on-the-fly kernel generation)
###
### Minimum Inputs: 
###		
###		- separate source/target input heat files: tab-separated, 3 columns each with <gene> <input heat> <sign (+/-)>
###		- a search pathway in .sif format (geneA <interaction> geneB)
###
### Outputs:
###
###		Creates a directory in the current working directory, and writes all output to that
###		Information and warnings are logged to standard error


import os, sys
from collections import defaultdict
from optparse import OptionParser
parser = OptionParser()
parser.add_option("-k","--kernel",dest="kernel",action="store",type="string",default=None,help="\
Pre-computed heat diffusion kernel in tab-delimited form. Should have both a header and row labels. \
The program will attempt to use scipy to generate a kernel if none is supplied.")
parser.add_option("-n","--network",dest="network",action="store",default=None,help="\
.sif network file for the curated pathway to search. <source>   <(-a>,-a|,-t>,-t|,-component>)> <target>")
parser.add_option("-p","--permute",dest="permute",action="store",default=50,type="int",help="\
Number of random permutations performed for significance analysis (default 50)")
parser.add_option("--pagerank",dest="pagerank",action="store_true",default=False,
help="Use Personalized PageRank to Diffuse")
parser.add_option("-o","--output",dest="output",action="store",default="")
parser.add_option("-c","--consider",dest="consider_top",action="store",default=3)
# data specific for patient-specific networks here
parser.add_option("-e","--expr_matrix",dest="expr_matrix",action="store",default=None,type="string",help="\
Normal-subtracted Gene Expression Matrix, rows = Genes, columns = Samples")
parser.add_option("--d_expr",dest="d_expr",action="store",default=None,type="string",help="\
List of significantly differentially expressed genes, along with log-FC or FC values (i.e. by edgeR for\
RNA-Seq or SAM for microarray data. Generated by a sample-dichotomy of interest")
parser.add_option("--concensus",dest="concensus",action="store",default=None, help="Subsample inputs and generate a concensus network for TieDIE (subsample rate:num samples) i.e. (0.85:100)")
# optional inputs here:
(opts, args) = parser.parse_args()

# local imports assume the directory structure from github . 
sys.path.append(os.path.dirname(sys.argv[0])+'/../lib')
from kernel import Kernel
from distance import ProbDistance
from distributions import Dist
from ppr import PPrDiffuser
from permute import NetBalancedPermuter
from tiedie_util import *
from itertools import combinations

if opts.kernel is None:
	sys.stderr.write("Warning: No kernel file supplied, will use SCIPY to compute the matrix exponential, t=0.1...\n")
	from kernel_scipy import SciPYKernel

def min(vals):
	min = vals[0]
	for v in vals:
		if v < min:
			min = v

	return min

# NOTE should all these functions be moved to a file in lib, as they currently
# appear in multiple files in bin? --ESR
def getProduct(diffused):
	gene_scores = {}
	for file in diffused:
		# a hash of hashes: file is the index
		for (gene, heat) in diffused[file].iteritems():
			if gene not in gene_scores:
				gene_scores[gene] = []
			gene_scores[gene].append(heat)

	gene_products = {}
	for gene in gene_scores:
		product = 1
		for v in gene_scores[gene]:
			product *= v
		gene_products[gene] = product
	
	return gene_products
 
def getMinHeats(consider_top, diffused):
	"""
	Gets the minimum heats for all genes, from a number of diffused heat vectors.

	Input:
        consider_top -- ???
		diffused = { 'set':{'gene1':heat1, 'gene2':...} -- what?

	Returns:
		A minimum-heat vector over all genes represented as a dict where the
        name of the gene is the key
			
	"""

	gene_scores = {}
	for file in diffused:
		# a hash of hashes: file is the index
		for (gene, heat) in diffused[file].iteritems():
			if gene not in gene_scores:
				gene_scores[gene] = []
			gene_scores[gene].append(heat)

  
	min_gene_values = {} 
	for gene in gene_scores:
		values = gene_scores[gene]
		# get the top X
		min_gene_values[gene] = min(sorted(values, reverse=True)[0:consider_top])

	return min_gene_values

def extractSubnetwork(input_heats, diffused_heats, size_control):
	"""
	Generate a spanning subnetwork from the supplied inputs, diffused heats and 
	size control cutoff

	Input:
		- input heats (in what format?)
		- diffused input heats (in what format?)
		- size control factor (explain what this means?)

	Returns: tuple containing the following elements
		- spanning network in 'network[node] = [(edge_type, other_node)]' format
		- set of node ID strings in that network
        - linker heats
        - linker cutoff
	"""

	linker_cutoff = None
	linker_nodes = None
	linker_scores = None

	# get linker heats as a function of input sets
	linker_heats = None
	if not opts.product:
		linker_heats = getMinHeats(int(opts.consider_top), diffused_heats)
	else:
		linker_heats = getProduct(diffused_heats)

	EPSILON = 0.0001
	linker_cutoff = None
	score = None
	linkers = set()
	linker_scores = {}
	for (l,h) in sorted(linker_heats.iteritems(), key=operator.itemgetter(1), reverse=True):
        # NOTE why is this function returning the last value of linker_cutoff?
		linker_cutoff = h-EPSILON
		score, size_frac = scoreLinkersMulti(input_heats, linker_heats,
                linker_cutoff, size_control)
		linkers.add(l)
		linker_scores[l] = h
		if size_frac > 1:
			break

	# set of input heats
	ugraph = None
	# USE JUST LINKER GENES
	active_nodes = set(linkers)
	ugraph = connectedSubnets(network, active_nodes)
	if len(ugraph) == 0:
		raise Exception("Couldn't find any linking graph at this size setting!")
	subnet_soln = mapUGraphToNetwork(ugraph, network)
	
	subnet_soln_nodes = set() # set of node ID strings contained in the subnet
	for s in subnet_soln:
		subnet_soln_nodes.add(s)
		for (i,t) in subnet_soln[s]:
			subnet_soln_nodes.add(t)

	return (subnet_soln, subnet_soln_nodes, linker_heats, linker_cutoff)

def getPVAL(setA, setB, network, diffuser, writeToFile=None):
    # TODO add docstring!

	# Instantiate to perform random permutations of the upstream heats, using the topology of the network
	# to correct for node degree
	perObj_setA = NetBalancedPermuter(network, setA)
	perObj_setB = NetBalancedPermuter(network, setB)
	# Perform the number random permutations specified (default 1000)
	permutedHeats_setA = perObj_setA.permute(PERMUTE)
	permutedHeats_setB = perObj_setB.permute(PERMUTE)
	# scores for each of the permutations
	permuted_kldiv = []

	# diffuse original heats, get the KL divergence between vectors
	setA_diffused = diffuser.diffuse(setA, reverse=False)
	setB_diffused = diffuser.diffuse(setB, reverse=True)
	real_kldiv = ProbDistance.getSymmetricMeasure(setA_diffused, setB_diffused)

	# permuted upstream 'sources'
	for heats in permutedHeats_setA:
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_heats =  diffuser.diffuse(heats)
		# get the angle between these
		kldiv = ProbDistance.getSymmetricMeasure(setB_diffused, diffused_heats)
		permuted_kldiv.append(kldiv)
	# permuted downstream 'targets'
	for heats in permutedHeats_setB:
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_heats =  diffuser.diffuse(heats)
		# get the angle between these
		kldiv = ProbDistance.getSymmetricMeasure(setA_diffused, diffused_heats)
		permuted_kldiv.append(kldiv)

	# just calculate the number of permuted sets that scored better than the real input set. 
	no_lte = 0.0
	for val in sorted(permuted_kldiv):
		if val <= real_kldiv:
			no_lte += 1
		else:
			break

	if writeToFile:
		fh = open(writeToFile, 'w')
		fh.write("Real:"+str(real_kldiv)+"\n")
		fh.write("Score\n")
		for val in permuted_kldiv:
			fh.write(str(val)+"\n")
		fh.close()

	# fit to lognormal, get p-value
	empirical_pval = (no_lte+1)/(PERMUTE*2+1)
	gauss_fit_pval = Dist.fitLogNorm(permuted_kldiv, real_kldiv)

	return (empirical_pval, gauss_fit_pval)

# Set the number of random permutations for the null-model test
PERMUTE = None
try:
	PERMUTE = int(opts.permute)
except:
	raise Exception("Error: bad input format option: --permute")

# parse network file: use for input validation if heat nodes are not in network
sys.stderr.write("Parsing Network File..\n")
network = parseNet(opts.network)
network_nodes = getNetworkNodes(network)

input_heats = {}
input_actions = {}
input_nodes = set()
for file in args:
	input_heat, signs = parseHeats(file)
	input_heats[file] = input_heat
	input_actions[file] = signs
	for gene in input_heats[file]:
		input_nodes.add(gene)

# Set the desired relative size of the linker set of genes to be found by the algorithm
size_control = float(opts.size)

out_prefix = "."
output_folder = out_prefix+"/TieDIE.Multi_"+str(opts.output)+"_RESULT_size="+str(opts.size)
if opts.pagerank:
	output_folder += "_PAGERANK"

if not os.path.exists(output_folder):
	os.mkdir(output_folder)

#
# Diffusion Step:
#	Load the heat diffusion kernel and perform a kernel-multiply, or alternatively use supplied
# 	page-rank diffused vectors
#

if opts.pagerank:
	# use PageRank to diffuse heats: create a diffuser object to perform this step
	diffuser = PPrDiffuser(network)
else:
	if opts.kernel is not None:
		sys.stderr.write("Loading Heat Diffusion Kernel..\n")
		# load a heat diffusion kernel to perform diffusion
		diffuser = Kernel(opts.kernel)
	else:
		sys.stderr.write("Using SCIPY to compute the matrix exponential, t=0.1...\n")
		# No kernel supplied: use SCIPY to generate a kernel on the fly, and then use it
		# for subsequent diffusion operations
		diffuser = SciPYKernel(opts.network)

print "Diffusing Heats..."
diffused_heats = {}
for file in input_heats:
	diffused_heats[file] = diffuser.diffuse(input_heats[file])

# TODO Run Master Regulator analysis on the differential expression list, if
# supplied. Find master regulators and add them to the input set list. Save the
# list of master-regulators in a file and continue with the analysis. 

# compute all pairwise distances, validate the input sets first
print "Computing pairwise p-values between sets"
print "\t".join(["Input A", "Input B", "Fitted Pval", "Empirical Pval"])
for input1, input2 in combinations(input_heats, 2):

	# print kl divergence
	real_kldiv = ProbDistance.getSymmetricMeasure(diffused_heats[input1],
            diffused_heats[input2])
	#print "\t".join([input1, input2, str(real_kldiv)])
	output_file = output_folder+"/"+input1+":"+input2+".dist.txt"
	empirical_p, fitted_p = getPVAL(input_heats[input1],
            input_heats[input2], network, diffuser, output_file)
	print "\t".join([input1, input2, str(fitted_p), str(empirical_p)])

# TODO Run TieDIE on the cohort data, over a range of size-options over a range
# of subsampled inputs. Store a distribution of heats for each node and each 
# edge over subsampling, and the inclusion fractions over the range of size
# options. Save this. This part is for Ed to do.

# TODO un-hardcode these variables
sizes = range(1,10) # the sizes that should be tried
num_subsamples = 1000 # the number of subsamples on which TieDIE should be run

# TODO consider using arrays instead of dicts for some of these
# node_heats[i][j] is the diffused heat at node j in subsample i 
node_heats = []
# edge_heats[(i,p,q)] is the heat on edge from node p to node q in subsample i
edge_heats = defaultdict(float)
# node_inclusion_fractions[(s,n)] is the fraction of subsamples at size s in
# which node n was included in the subnetwork
node_inclusion_fractions = defaultdict(float)
# edge_inclusion_fractions[(s,p,q)] is the fraction of subsamples at size s in
# which the edge from node p to node q was included in the subnetwork
edge_inclusion_fractions = defaultdict(float)

for i in range(num_subsamples):
    # TODO create subsample and store in subsample_input_heats
    subsample_input_heats = None

    subsample_diffused_heats = diffuser.diffuse(subsample_input_heats)
    node_heats.append(subsample_diffused_heats)

    # TODO store edge heats
    # how do you get edge heats from the node heats? -ESR

    for size in sizes:
        subnet, subnet_nodes, temp, temp = extractSubnetwork(
                subsample_input_heats, subsample_diffused_heats, size)

        # add a fraction to all nodes appearing in this subnetwork
        for node in subnet_nodes:
            node_inclusion_fractions[(size,node)] += 1.0/num_subsamples
        
        # add a fraction to all edges appearing in this subnetwork
        for from_node, (inter, to_node) in subnet.iteritems():
            edge_inclusion_fractions[(size,from_node,to_node)] +=\
                    1.0/num_subsamples

# TODO Generate patient-specific networks by running TieDIE on each patient's
# data, setting the input size to parity and doing a simple depth-first
# traversal from mutations to TFs. Store each network in a file.
