#!/usr/bin/env python

###
### TieDIE: Tied Diffusion for Network Discovery
###
###	Version: 
###
###		Multiple (>= 3 inputs) Development Version
###
###	Authors: 
###
###		Evan Paull (epaull@soe.ucsc.edu)
###
###	Requirements:
###
### 	python 2.7.X
###		numpy 1.7+ (with pre-computed kernels)
###		scipy 0.12+ (for on-the-fly kernel generation)
###
### Minimum Inputs: 
###		
###		- separate source/target input heat files: tab-separated, 3 columns each with <gene> <input heat> <sign (+/-)>
###		- a search pathway in .sif format (geneA <interaction> geneB)
###
### Outputs:
###
###		Creates a directory in the current working directory, and writes all output to that
###		Information and warnings are logged to standard error


import os, sys
from collections import defaultdict
from optparse import OptionParser
parser = OptionParser()
parser.add_option("-k","--kernel",dest="kernel",action="store",type="string",default=None,help="\
Pre-computed heat diffusion kernel in tab-delimited form. Should have both a header and row labels. \
The program will attempt to use scipy to generate a kernel if none is supplied.")
parser.add_option("-n","--network",dest="network",action="store",default=None,help="\
.sif network file for the curated pathway to search. <source>   <(-a>,-a|,-t>,-t|,-component>)> <target>")
parser.add_option("-p","--permute",dest="permute",action="store",default=50,type="int",help="\
Number of random permutations performed for significance analysis (default 50)")
parser.add_option("--pagerank",dest="pagerank",action="store_true",default=False,
help="Use Personalized PageRank to Diffuse")
parser.add_option("-o","--output",dest="output",action="store",default="")
parser.add_option("-c","--consider",dest="consider_top",action="store",default=3)
# data specific for patient-specific networks here
parser.add_option("--d_expr",dest="d_expr",action="store",default=None,type="string",help="\
List of significantly differentially expressed genes, along with log-FC or FC values (i.e. by edgeR for\
RNA-Seq or SAM for microarray data. Generated by a sample-dichotomy of interest")
parser.add_option("-m","--min_hub",dest="min_hub",action="store",default=10,type="int",help=\
"minimum number of genes in regulon to consider a TF")

# optional inputs here:
(opts, args) = parser.parse_args()

# local imports assume the directory structure from github . 
sys.path.append(os.path.dirname(sys.argv[0])+'/../lib')
from kernel import Kernel
from distance import ProbDistance
from distributions import Dist
from ppr import PPrDiffuser
from permute import NetBalancedPermuter
import tiedie_util
from tiedie_util import *
from itertools import combinations
from master_reg import ActivityScores
from consensus import *

if opts.kernel is None:
	sys.stderr.write("Warning: No kernel file supplied, will use SCIPY to compute the matrix exponential, t=0.1...\n")
	from kernel_scipy import SciPYKernel

# NOTE should some of these functions be moved to a file in lib, as they
# currently appear in multiple files in bin? --ESR
def getProduct(diffused):
	gene_scores = {}
	for file in diffused:
		# a hash of hashes: file is the index
		for (gene, heat) in diffused[file].iteritems():
			if gene not in gene_scores:
				gene_scores[gene] = []
			gene_scores[gene].append(heat)

	gene_products = {}
	for gene in gene_scores:
		product = 1
		for v in gene_scores[gene]:
			product *= v
		gene_products[gene] = product
	
	return gene_products
 
def getMinHeats(consider_top, diffused):
	"""
	Gets the minimum heats for all genes, from a number of diffused heat vectors.

	Input:
        consider_top -- ???
		diffused = { 'set':{'gene1':heat1, 'gene2':...} -- what?

	Returns:
		A minimum-heat vector over all genes represented as a dict where the
        name of the gene is the key
			
	"""

	gene_scores = {}
	for file in diffused:
		# a hash of hashes: file is the index
		for (gene, heat) in diffused[file].iteritems():
			if gene not in gene_scores:
				gene_scores[gene] = []
			gene_scores[gene].append(heat)

  
	min_gene_values = {} 
	for gene in gene_scores:
		values = gene_scores[gene]
		# get the top X
		min_gene_values[gene] = min(sorted(values, reverse=True)[0:consider_top])

	return min_gene_values

def extractSubnetwork(input_heats, diffused_heats, size_control):
	"""
	Generate a spanning subnetwork from the supplied inputs, diffused heats and 
	size control cutoff

	Input:
		- input heats (in what format?)
		- diffused input heats (in what format?)
		- size control factor (explain what this means?)

	Returns: tuple containing the following elements
		- spanning network in 'network[node] = [(edge_type, other_node)]' format
		- set of node ID strings in that network
        - linker heats
        - linker cutoff
	"""

	linker_cutoff = None
	linker_nodes = None
	linker_scores = None

	# get linker heats as a function of input sets
	linker_heats = None
	if not opts.product:
		linker_heats = getMinHeats(int(opts.consider_top), diffused_heats)
	else:
		linker_heats = getProduct(diffused_heats)

	EPSILON = 0.0001
	linker_cutoff = None
	score = None
	linkers = set()
	linker_scores = {}
	for (l,h) in sorted(linker_heats.iteritems(), key=operator.itemgetter(1), reverse=True):
        # NOTE why is this function returning the last value of linker_cutoff?
		linker_cutoff = h-EPSILON
		score, size_frac = scoreLinkersMulti(input_heats, linker_heats,
                linker_cutoff, size_control)
		linkers.add(l)
		linker_scores[l] = h
		if size_frac > 1:
			break

	# set of input heats
	ugraph = None
	# USE JUST LINKER GENES
	active_nodes = set(linkers)
	ugraph = connectedSubnets(network, active_nodes)
	if len(ugraph) == 0:
		raise Exception("Couldn't find any linking graph at this size setting!")
	subnet_soln = mapUGraphToNetwork(ugraph, network)
	
	subnet_soln_nodes = set() # set of node ID strings contained in the subnet
	for s in subnet_soln:
		subnet_soln_nodes.add(s)
		for (i,t) in subnet_soln[s]:
			subnet_soln_nodes.add(t)

	return (subnet_soln, subnet_soln_nodes, linker_heats, linker_cutoff)

def getPVAL(setA, setB, network, diffuser, writeToFile=None):
    # TODO add docstring!

	# Instantiate to perform random permutations of the upstream heats, using the topology of the network
	# to correct for node degree
	perObj_setA = NetBalancedPermuter(network, setA)
	perObj_setB = NetBalancedPermuter(network, setB)
	# Perform the number random permutations specified (default 1000)
	permutedHeats_setA = perObj_setA.permute(PERMUTE)
	permutedHeats_setB = perObj_setB.permute(PERMUTE)
	# scores for each of the permutations
	permuted_kldiv = []

	# diffuse original heats, get the KL divergence between vectors
	setA_diffused = diffuser.diffuse(setA, reverse=False)
	setB_diffused = diffuser.diffuse(setB, reverse=True)
	real_kldiv = ProbDistance.getSymmetricMeasure(setA_diffused, setB_diffused)

	# permuted upstream 'sources'
	for heats in permutedHeats_setA:
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_heats =  diffuser.diffuse(heats)
		# get the angle between these
		kldiv = ProbDistance.getSymmetricMeasure(setB_diffused, diffused_heats)
		permuted_kldiv.append(kldiv)
	# permuted downstream 'targets'
	for heats in permutedHeats_setB:
		# diffuse the permuted scores, then find the Relevance score for that permuted set 
		diffused_heats =  diffuser.diffuse(heats)
		# get the angle between these
		kldiv = ProbDistance.getSymmetricMeasure(setA_diffused, diffused_heats)
		permuted_kldiv.append(kldiv)

	# just calculate the number of permuted sets that scored better than the real input set. 
	no_lte = 0.0
	for val in sorted(permuted_kldiv):
		if val <= real_kldiv:
			no_lte += 1
		else:
			break

	if writeToFile:
		fh = open(writeToFile, 'w')
		fh.write("Real:"+str(real_kldiv)+"\n")
		fh.write("Score\n")
		for val in permuted_kldiv:
			fh.write(str(val)+"\n")
		fh.close()

	# fit to lognormal, get p-value
	empirical_pval = (no_lte+1)/(PERMUTE*2+1)
	gauss_fit_pval = Dist.fitLogNorm(permuted_kldiv, real_kldiv)

	return (empirical_pval, gauss_fit_pval)

# Set the number of random permutations for the null-model test
# FIXME this functionality is built in to argparse
PERMUTE = None
try:
	PERMUTE = int(opts.permute)
except:
	raise Exception("Error: bad input format option: --permute")

# parse network file: use for input validation if heat nodes are not in network
sys.stderr.write("Parsing Network File..\n")
network = parseNet(opts.network)
network_nodes = getNetworkNodes(network)

input_heats = {}
input_actions = {}
input_nodes = set()
for file in args:
	input_heat, signs = parseHeats(file)
	input_heats[file] = input_heat
	input_actions[file] = signs
	for gene in input_heats[file]:
		input_nodes.add(gene)

out_prefix = "."
output_folder = out_prefix+"/TieDIE.CONCENSUS"
if opts.pagerank:
	output_folder += "_PAGERANK"

if not os.path.exists(output_folder):
	os.mkdir(output_folder)

#
# Diffusion Step:
#	Load the heat diffusion kernel and perform a kernel-multiply, or alternatively use supplied
# 	page-rank diffused vectors
#

if opts.pagerank:
	# use PageRank to diffuse heats: create a diffuser object to perform this step
	diffuser = PPrDiffuser(network)
else:
	if opts.kernel is not None:
		sys.stderr.write("Loading Heat Diffusion Kernel..\n")
		# load a heat diffusion kernel to perform diffusion
		diffuser = Kernel(opts.kernel)
	else:
		sys.stderr.write("Using SCIPY to compute the matrix exponential, t=0.1...\n")
		# No kernel supplied: use SCIPY to generate a kernel on the fly, and then use it
		# for subsequent diffusion operations
		diffuser = SciPYKernel(opts.network)

# normalize input heats to sum to 1
for input in input_heats:
	total = 0.0
	for (g, h) in input_heats[input].items():
		total += abs(float(h))
	norm_const = 1.0/total
	# normalize	in place
	for (g, h) in input_heats[input].items():
		input_heats[input][g] = h*norm_const

print "Diffusing Heats..."
diffused_heats = {}
for file in input_heats:
	diffused_heats[file] = diffuser.diffuse(input_heats[file])

tfs_heats = None
# Run Master Regulator analysis on the differential expression list, if
# supplied. Find master regulators and add them to the input set list. Save the
# list of master-regulators in a file and continue with the analysis. 
if opts.d_expr:
	tfs_heats = ActivityScores.findRegulators(network, opts.d_expr, min_hub=opts.min_hub)

	# output inferred tf heats:
	fh = open(output_folder+"/tfs.inferred", 'w')
	for (g, h) in tfs_heats.items():
		fh.write(g+"\t"+str(h)+"\n")
	fh.close()

	# diffuse heats from these tfs
	diffused_heats["tfs"] = diffuser.diffuse(tfs_heats)

# compute all pairwise distances, validate the input sets first
print "Computing pairwise p-values between sets"
print "\t".join(["Input A", "Input B", "Fitted Pval", "Empirical Pval"])
for input1, input2 in combinations(input_heats, 2):

	# print kl divergence
	real_kldiv = ProbDistance.getSymmetricMeasure(diffused_heats[input1],
            diffused_heats[input2])
	#print "\t".join([input1, input2, str(real_kldiv)])
	output_file = output_folder+"/"+input1+":"+input2+".dist.txt"
	empirical_p, fitted_p = getPVAL(input_heats[input1],
            input_heats[input2], network, diffuser, output_file)
	print "\t".join([input1, input2, str(fitted_p), str(empirical_p)])

# pairwise distances between TFS: save results and print to stdout
if tfs_heats:
	i = 1
	for input in input_heats:
		output_file = output_folder+"/"+str(i)+":"+"tfs"+".dist.txt"
		empirical_p, fitted_p = getPVAL(input_heats[input], tfs_heats, network, diffuser, output_file)
		print "\t".join([input, "tfs", str(fitted_p), str(empirical_p)])

print "done"				

# Run TieDIE on the cohort data, over a range of size-options over a range
# of subsampled inputs. Store a distribution of heats for each node and each 
# edge over subsampling, and the inclusion fractions over the range of size
# options. Save this. This part is for Ed to do.

# subsample both upstream/downstream inputs	
options = {}
options['subsample_which'] = "u"
options['size'] = [i/2.0 for i in range(1,10)]
options['network_file'] = opts.network
samples = 100
rate = 0.85

# by convention, the first input should be the mutation data
upstream_heats = input_heats[input_heats.keys()[0]]

# do a full TieDIE run: save heat values
# write the cytoscape output file
print "Running TieDIE with Standard Settings..."
std_options = copy.copy(options)
std_options['size'] = 1.0
up_heats_diffused = diffuser.diffuse(upstream_heats, reverse=False)
down_heats_diffused = diffuser.diffuse(tfs_heats, reverse=True)
subnet_soln, subnet_soln_nodes, alpha_score, linker_scores = \
        tiedie_util.extractSubnetwork(upstream_heats, tfs_heats,
                up_heats_diffused, down_heats_diffused, network, std_options)
writeNAfile(output_folder+"/heats.NA", linker_scores, "LinkerHeats")
writeHEATS(output_folder+"/heats.tab", linker_scores)
print "done"

# get statistics--nodes and edges
print "Generating consensus TieDIE networks..."
cNet = ConsensusNetwork(network, diffuser)
cNet.generate(upstream_heats, tfs_heats, samples, rate, options)
edge_frequencies, node_frequencies, node_heat_distributions = cNet.getStats()

# write out edge/node fractions
edge_fh = open(output_folder+"/edge_frequencies.txt", 'w')
node_fh = open(output_folder+"/node_frequencies.txt", 'w')
node_heats = open(output_folder+"/node_heats.txt", 'w')
for edge in edge_frequencies:
	edge_fh.write("\t".join(edge)+"\t"+str(edge_frequencies[edge])+"\n")
edge_fh.close()
for node in node_frequencies:
	node_fh.write(node+"\t"+str(node_frequencies[node])+"\n")
node_fh.close()
for node in node_heat_distributions:
	node_heats.write(node+"\t"+"\t".join([str(v) for v in node_heat_distributions[node]])+"\n")
node_heats.close()
print "done"
