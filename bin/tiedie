#!/usr/bin/env python

###
### TieDIE: Tied Diffusion for Network Discovery
###
###	Version: 
###
###		1.0
###
###	Authors: 
###
###		Evan Paull (epaull@soe.ucsc.edu)
###
###	Requirements:
###
### 	python 2.7.X
###		numpy 1.7+ (with pre-computed kernels)
###		scipy 0.12+ (for on-the-fly kernel generation)
###
### Minimum Inputs: 
###		
###		- separate source/target input heat files: tab-separated, 3 columns each with <gene> <input heat> <sign (+/-)>
###		- a search pathway in .sif format (geneA <interaction> geneB)
###
### Outputs:
###
###		Creates a directory in the current working directory, and writes all output to that
###		Information and warnings are logged to standard error


import os, sys
from collections import defaultdict
from optparse import OptionParser
parser = OptionParser()
parser.add_option("-u","--up_heats",dest="up_heats",action="store",type="string",default=None,help="\
File with upstream heats: <gene>    <input heat (0-100)>    <sign (+/-)>")
parser.add_option("-d","--down_heats",dest="down_heats",action="store",type="string",default=None,help="\
File with downstream heats: <gene>    <input heat (0-100)>    <sign (+/-)>")
parser.add_option("-k","--kernel",dest="kernel",action="store",type="string",default=None,help="\
Pre-computed heat diffusion kernel in tab-delimited form. Should have both a header and row labels. \
The program will attempt to use scipy to generate a kernel if none is supplied.")
parser.add_option("--pcst",dest="pcst",action="store_true",default=False,help="\
Use the Prize-Collecting Steiner Tree Formulation to Generate a Connecting Subnetwork (Bionet must be installed)")
parser.add_option("-n","--network",dest="network",action="store",default=None,help="\
.sif network file for the curated pathway to search. <source>   <(-a>,-a|,-t>,-t|,-component>)> <target>")
parser.add_option("-s","--size",dest="size",action="store",default=1,type="float",help="\
Network size control factor (default 1)")
parser.add_option("-a","--alpha",dest="alpha",action="store",default=None,help="Linker Cutoff \
(overrides the Size factor)")
parser.add_option("-c","--depth",dest="depth",action="store",default=3,type="int",help="\
Search depth for causal paths (default 3)")
parser.add_option("-p","--permute",dest="permute",action="store",default=1000,type="int",help="\
Number of random permutations performed for significance analysis (default 1000)")
parser.add_option("--pagerank",dest="pagerank",action="store_true",default=False,
help="Use Personalized PageRank to Diffuse")
parser.add_option("--all_paths",dest="all_paths",action="store_true",default=False)
parser.add_option("--permute_downstream",dest="permute_down",action="store_true",default=False,help="\
permute the downstream set instead of the upstream")
parser.add_option("--concensus",dest="concensus",action="store",default=None, help="Subsample inputs and generate a concensus network for TieDIE (subsample rate:num samples) i.e. (0.85:100)")
(opts, args) = parser.parse_args()

# local imports assume the directory structure from github . 
sys.path.append(os.path.dirname(sys.argv[0])+'/../lib')
from kernel import Kernel
from ppr import PPrDiffuser
from permute import NetBalancedPermuter
from tiedie_util import *
from concensus import ConcensusNetwork

if opts.kernel is None:
	sys.stderr.write("Warning: No kernel file supplied, will use SCIPY to compute the matrix exponential, t=0.1...\n")
	from kernel_scipy import SciPYKernel

def findConsistentPaths(up_signs, down_signs, searchNetwork, output_folder, output):

	"""
		Filter the heat-generated network by searching for all directed paths from each
		source to each target gene. 

		Input:
			- up_signs: hash with up/down signs for each upstream node
			- down_signs: hash with up/down signs for each downstream node
			- searchNetwork: heat derived subnetwork to use as the basis for the depth-first search
			- output_folder: write output networks under this directory
			
		Options:
			- output: flag to write output to the specified folder

		Returns:

			- TP: true positive links (count)
			- FP: false positive links, if supplied with 'dummy' edges for the precision/recall test
			- validated: an edge list of validated edges
			

	"""
	# States can be classified as either a gain or loss of function, or up/down regulated. 
	# If a gene is in both source and target sets, the state of the source (perturbation) set
	# takes precedence. 
	gene_states, t_states = classifyState(up_signs, down_signs)
	# store the set of validated edges here
	validated = set()
	down_set = set(down_signs.keys())
	# if we're doing a randomized link analysis, keep track of TP and FP scores
	TP = 0
	FP = 0
	# for each upstream 'perturbed' gene, do a search for possible targets, following directional links
	for source in up_signs:
		action = gene_states[source]
		falsePaths = []
		truePaths = []	
		edges_this_source = set()
		# perform a recursive DFS search for any of this target set, following directional links and stopping when a target is hit
		# keep track of True/False paths--necessary if performing the precision/recall evaluation--, defined as when a path contains 
		# one or more injected false edges. 
		searchDFS(source, action, edges_this_source, set(), down_set, searchNetwork, gene_states, t_states, search_depth, truePaths, falsePaths, False)
		for edge in edges_this_source:
			validated.add(edge)
		TP += len(truePaths)	
		FP += len(falsePaths)	

		# write causal networks to the output folder
		if output:
			out_file = output_folder+"/"+source+".cn.sif"
			sys.stderr.write("Writing Single Causal Neighborhood to "+out_file+"\n")
			writeEL(edges_this_source, source, down_set, out_file)

	if output:	
		out_file = output_folder+"/tiedie.cn.sif"
		sys.stderr.write("Writing Full Causal Neighborhood to "+out_file+"\n")
		# this is the union of all individual source networks
		writeEL(validated, "ALL", down_set, out_file)

	return (TP, FP, validated)

def scoreSubnet(subnet_soln_nodes, up_heats, down_heats, report_fh):
	"""
		Score Sets According to a Compactness Score that weighs the coverage of source and target sets
		while penalizing for the number of linker nodes needed to connected them.
	"""
	S = set(up_heats.keys())
	T = set(down_heats.keys())
	# C is the connecting set: linker genes that are in neither the source, nor target sets
	C = subnet_soln_nodes.difference(S).difference(T)
	U = S.union(T)
	# record the 'captured' nodes from source and target sets, in the network solution
	Sr = S.intersection(subnet_soln_nodes)
	Tr = T.intersection(subnet_soln_nodes)
	PENALTY_CONST = 0.1
	# penalize the connecting gene set, as a fraction of the total network size (by # nodes)
	penalty = (float(len(C))/len(U))*PENALTY_CONST
	score = float(len(Sr))/(len(S)*2) + float(len(Tr))/(len(T)*2) - penalty

	# output to report file
	report_fh.write(str(float(len(Sr))/len(S))+"\t"+"of source nodes"+str(len(Sr))+" out of "+str(len(S))+"\n")
	report_fh.write(str(float(len(Tr))/len(T))+"\t"+"of target nodes"+str(len(Tr))+" out of "+str(len(T))+"\n")
	report_fh.write("And "+str(len(C))+" connecting nodes\n")

	return score

# Set the number of random permutations for the null-model test
PERMUTE = None
try:
	PERMUTE = int(opts.permute)
except:
	raise Exception("Error: bad input format option: --permute")

# parse network file: use for input validation if heat nodes are not in network
sys.stderr.write("Parsing Network File..\n")
network = parseNet(opts.network)
network_nodes = getNetworkNodes(network)

# parse the input heats for both upstream and downstream sets, and record the signs
# representing the putative perturbation effect, or transcriptional activity
up_heats, up_signs = parseHeats(opts.up_heats, network_nodes)
down_heats, down_signs = parseHeats(opts.down_heats, network_nodes)
# Set the desired relative size of the linker set of genes to be found by the algorithm
size_control = float(opts.size)
# Maximum search depth for directed path (source->target) search
search_depth = int(opts.depth)

# set the output folder for the reports and networks, create the directory
out_prefix = os.path.dirname(opts.up_heats)
if out_prefix == "":
	out_prefix = "."
output_folder = out_prefix+"/TieDIE_RESULT_size="+str(opts.size)+"_depth="+str(opts.depth)
if opts.alpha:
	output_folder = out_prefix+"/TieDIE_RESULT_"+str(opts.alpha)
if opts.pagerank:
	output_folder += "_PAGERANK"

if not os.path.exists(output_folder):
	os.mkdir(output_folder)

#
# Diffusion Step:
#	Load the heat diffusion kernel and perform a kernel-multiply, or alternatively use supplied
# 	page-rank diffused vectors
#

if opts.pagerank:
	# use PageRank to diffuse heats: create a diffuser object to perform this step
	diffuser = PPrDiffuser(network)
else:
	if opts.kernel is not None:
		sys.stderr.write("Loading Heat Diffusion Kernel..\n")
		# load a heat diffusion kernel to perform diffusion
		diffuser = Kernel(opts.kernel)
	else:
		sys.stderr.write("Using SCIPY to compute the matrix exponential, t=0.1...\n")
		# No kernel supplied: use SCIPY to generate a kernel on the fly, and then use it
		# for subsequent diffusion operations
		diffuser = SciPYKernel(opts.network)

sys.stderr.write("Diffusing Heats...\n")
# Separately perform diffusion for each input set
up_heats_diffused = diffuser.diffuse(up_heats, reverse=False)
down_heats_diffused = diffuser.diffuse(down_heats, reverse=True)

# Extract a subnetwork solution from the diffused heats, the set of nodes found, the Relevance score for this network solution
# and the heat scores for all linker genes
options = {}
options['alpha'] = opts.alpha
options['size'] = opts.size
options['network_file'] = opts.network
options['pcst'] = opts.pcst
subnet_soln, subnet_soln_nodes, alpha_score, linker_scores = extractSubnetwork(up_heats, down_heats, up_heats_diffused, down_heats_diffused, network, options)

# 
# Generate linker stats and output
# 
out_degrees = getOutDegrees(subnet_soln)
sys.stderr.write("Writing network node stats to "+output_folder+"/node.stats\n")
out_file = output_folder+"/node.stats"
out = open(out_file, 'w')
out.write("NODE\tCONNECTING\tMIN_HEAT\tOUT_DEGREE\n")
# is each node in the solution a source, linker or target? Track in node_state
node_types = {}
for node in subnet_soln_nodes:
	out_deg = out_degrees[node]
	linker_heat = linker_scores[node]
	connecting = "0" 
	if node in up_heats:
		# source node = 1
		node_types[node] = 1	
	elif node in down_heats:
		# target node = -1
		node_types[node] = -1	

	# assign a connecting state if this node is not in the source or target sets
	if node not in up_heats:
		if down_heats is not None and node not in down_heats:
			connecting = "1"
			node_types[node] = 0
		
	out.write("\t".join([node, connecting, str(linker_heat), str(out_deg)])+"\n")
out.close()

# write the cytoscape output file
writeNAfile(output_folder+"/node_types.NA", node_types, "NodeTypes")

# write the cytoscape output file
writeNAfile(output_folder+"/heats.NA", linker_scores, "LinkerHeats")

# the complete TieDIE network, before the edge-filtering step
sys.stderr.write("Writing "+output_folder+"/tiedie.sif result \n")
writeNetwork(subnet_soln, output_folder+"/tiedie.sif")

# 
# Find logically consistent paths
#
TP, FP, validated = findConsistentPaths(up_signs, down_signs, subnet_soln, output_folder, True)

# Open the report file to log a summary of the results
report_file = output_folder+"/report.txt"
report_fh = open(report_file, 'w')

sys.stderr.write("Writing Report to "+report_file+" :compactness analysis\n")

#Score Sets According to a Compactness Score that weighs the coverage of source and target sets
#while penalizing for the number of linker nodes needed to connected them.
score = scoreSubnet(subnet_soln_nodes, up_heats, down_heats, report_fh)
report_fh.write("Compactness Score:"+str(score)+"\n")

sys.stderr.write("Running permutation tests... (could take several minutes for inputs of hundreds of genes @1000 permutations)\n")

# Instantiate to perform random permutations of the upstream heats, using the topology of the network
# to correct for node degree
perObj = None
stationary_input = None
if not opts.permute_down:
	perObj = NetBalancedPermuter(network, up_heats)
	stationary_input = (down_heats, down_heats_diffused)
else:
	# if the option is supplied, permute the downstream set instead
	perObj = NetBalancedPermuter(network, down_heats)
	stationary_input = (up_heats, up_heats_diffused)

# Perform the number random permutations specified (default 1000)
permutedHeats = perObj.permute(PERMUTE)
# scores for each of the permutations
permuted_scores = []
for heats in permutedHeats:
	# diffuse the permuted scores, then find the Relevance score for that permuted set 
	diffused_heats =  diffuser.diffuse(heats)
	cutoff, score = findLinkerCutoff(heats, down_heats, diffused_heats, down_heats_diffused, size_control)
	permuted_scores.append(score)

# write out the distribution for significance plot
sig_fh = open(output_folder+"/score.txt", 'w')
sig_fh.write(str(alpha_score)+"\n")
sig_fh.close()
sig_fh = open(output_folder+"/permuted_scores.txt", 'w')
for val in sorted(permuted_scores, reverse=True):
	sig_fh.write(str(val)+"\n")
sig_fh.close()

# just calculate the number of permuted sets that scored better than the real input set. 
no_gte = 0.0
for val in sorted(permuted_scores, reverse=True):
	if val >= alpha_score:
		no_gte += 1
	else:
		break

# Davison & Hinkley (1997): true empirical p-value is (r+1)/(n+1)
# Compare Relevance scores for the permuted samples to compute and empirical p-value for the 
# network. Write the scores to save for plotting. 
sys.stderr.write("Writing Report to "+report_file+" :empirical p-value...\n")
pval = (no_gte+1)/(PERMUTE+1)
report_fh.write("P-value: "+str(pval)+" (with "+str(PERMUTE)+" random permutations)\n")
report_fh.close()


# generate concensus network: optional
if opts.concensus:
	rate, samples = opts.concensus.split(":")	
	rate = float(rate)
	samples = int(samples)
	cNet = ConcensusNetwork(network, diffuser)
	cNet.generate(up_heats, down_heats, samples, rate, options)
	edge_frac, node_frac = cNet.getFractions()

	# write out edge/node fractions
	edge_fh = open(output_folder+"/edge_frequencies.txt", 'w')
	node_fh = open(output_folder+"/node_frequencies.txt", 'w')
	for edge in edge_frac:
		edge_fh.write("\t".join(edge)+"\t"+str(edge_frac[edge])+"\n")
	for node in node_frac:
		node_fh.write(node+"\t"+str(node_frac[node])+"\n")
	edge_fh.close()
	node_fh.close()
